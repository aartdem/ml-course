{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d472d849-7316-481b-a24d-86d4f3e6f3ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Нейронные сети\n",
    "- Найти данные в виде изображений для задачи классификации. Например, можно взять данные [отсюда](http://ufldl.stanford.edu/housenumbers/).\n",
    "- Реализовать классы, необходимые для построения сети со следующими слоями\n",
    "\t- FullyConnectedLayer\n",
    "\t- ReluLayer\n",
    "\t- FullyConnectedLayer\n",
    "- Использовать CrossEntropyLoss и L2-регуляризацию.\n",
    "- Обучить модель на тренировочных данных, подбирать параметры (особенно learning rate) на валидационной и оценить качество на тестовой. Анализировать графики train/val loss, проверять на каждом шаге корректность вычисления градиентов с помощью разностной оценки.\n",
    "- (**+2 балла**) Добавить Batch normalization.\n",
    "- (**+2 балла**) В качестве оптимизатор использовать один из: Momentum, RMSprop.\n",
    "- (**+1 балл**) Также реализовать оптимизатор Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c249a7-26cc-4699-8dff-b2cf5ff9dff4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Датасет\n",
    "Для решения задачи был взят датасет http://ufldl.stanford.edu/housenumbers/, а именно уже разделенные тренировочные и тестовые данные, без учета дополнительных данных. В репозитории этого датасета нет из-за его большого размера (~ 182MB train и 64MB test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6627b6f4-6970-42dd-b0d8-6b0b59564cb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet matplotlib\n",
    "!pip install --quiet numpy\n",
    "!pip install --quiet pandas \n",
    "!pip install --quiet torch\n",
    "!pip install --quiet torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9622deaf-0b59-4240-b1d1-b2d9d20f1e8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import SVHN\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b23ae5-4739-47ba-86fd-7c17245fdb43",
   "metadata": {},
   "source": [
    "Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7716346-7ebf-4f43-846f-af26d3ca1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "raw_train_data = SVHN(\n",
    "    root=\"../data/task07\", \n",
    "    split='train', \n",
    "    download=True, \n",
    "    transform=transform)\n",
    "\n",
    "test_data = SVHN(\n",
    "    root=\"../data/task07\", \n",
    "    split='test', \n",
    "    download=True, \n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba45c5f-3615-4364-bef9-d3e15bc7ffe8",
   "metadata": {},
   "source": [
    "Создание loader-ов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7e261c-7d0b-4a5f-9c7a-3520e982b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_size = len(raw_train_data)\n",
    "val_size = int(0.3 * raw_train_size)\n",
    "train_size = raw_train_size - val_size\n",
    "\n",
    "train_data, val_data = random_split(raw_train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b1c4e-6931-47ae-80a7-7614b0509483",
   "metadata": {},
   "source": [
    "## Слои сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0d6068d-b3f5-451d-ba5e-01e380268849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn(input_dim, output_dim) * torch.sqrt(torch.tensor(2.0 / input_dim)))\n",
    "        self.b = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self._x = x\n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, grad_out):\n",
    "        grad_in = grad_out @ self.W.t()\n",
    "        self.W.grad = self._x.t() @ grad_out\n",
    "        self.b.grad = grad_out.sum(dim=0)\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16cd0f62-3f27-42d9-aeb8-2d846fa4f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        self._x = x\n",
    "        return F.relu(x)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[self._x <= 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ef6ad-267c-4fae-b61d-142320234cc5",
   "metadata": {},
   "source": [
    "Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f35f367-7ef9-41d3-a5d8-341adfb875ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dim_hidden=128, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.layer_in = FullyConnectedLayer(3 * 32 * 32, dim_hidden)\n",
    "        self.activation = ReluLayer()\n",
    "        self.bn_layer = nn.BatchNorm1d(dim_hidden) if use_bn else None\n",
    "        self.layer_out = FullyConnectedLayer(dim_hidden, 10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        reshaped = inputs.view(-1, 3 * 32 * 32)\n",
    "        out = self.layer_in(reshaped)\n",
    "        out = self.activation(out)\n",
    "        if self.use_bn:\n",
    "            out = self.bn_layer(out)\n",
    "        out = self.layer_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3618cc-2738-4175-9f28-a4cd550961f3",
   "metadata": {},
   "source": [
    "## Оптимизатор Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "501eac4f-6460-4516-81d0-a79e7ab7beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.states = []\n",
    "        for p in self.params:\n",
    "            if p.requires_grad:\n",
    "                state = {\n",
    "                    'm': torch.zeros_like(p.data),\n",
    "                    'v': torch.zeros_like(p.data),\n",
    "                    't': 0\n",
    "                }\n",
    "                self.states.append(state)\n",
    "            else:\n",
    "                self.states.append(None)\n",
    "        \n",
    "        self.t = 0 \n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        beta1, beta2 = self.betas\n",
    "        \n",
    "        for param, state in zip(self.params, self.states):\n",
    "            if param.grad is None or state is None:\n",
    "                continue\n",
    "                \n",
    "            grad = param.grad.data\n",
    "            \n",
    "            if self.weight_decay != 0:\n",
    "                grad += self.weight_decay * param.data\n",
    "\n",
    "            state['m'] *= beta1\n",
    "            state['m'] += (1 - beta1) * grad\n",
    "            state['v'] *= beta2\n",
    "            state['v'] += (1 - beta2) * grad * grad\n",
    "            \n",
    "            m_hat = state['m'] / (1 - beta1 ** self.t)\n",
    "            v_hat = state['v'] / (1 - beta2 ** self.t)\n",
    "            \n",
    "            param.data -= self.lr * m_hat / (v_hat.sqrt() + self.eps)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.detach_()\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11b4fc-9a3a-4133-8b61-c837a665e2fd",
   "metadata": {},
   "source": [
    "## Обучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be7aee8-2105-4706-93cf-0f7066a9c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "devide = device = torch.device('cpu')\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "\n",
    "        train_loss.append(running_loss/len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "        \n",
    "        val_loss.append(val_running_loss/len(val_loader))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss[-1]}, Val Loss: {val_loss[-1]}\")\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95efe9d8-f88b-4581-b598-fd16e348a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training NeuralNetwork with lr=0.001, optimizer=adam, batch norm=False\n",
      "Epoch 1/10, Train Loss: 1.3897412085622327, Val Loss: 1.13796490915986\n",
      "Epoch 2/10, Train Loss: 0.9446058612214657, Val Loss: 0.9425010339811791\n",
      "Epoch 3/10, Train Loss: 0.820262461714911, Val Loss: 0.9216330870119638\n",
      "Epoch 4/10, Train Loss: 0.7414359329941861, Val Loss: 0.79617451469219\n",
      "Epoch 5/10, Train Loss: 0.6991120759314136, Val Loss: 0.7776406156982101\n",
      "Epoch 6/10, Train Loss: 0.6454887787562951, Val Loss: 0.7084675283106261\n",
      "Epoch 7/10, Train Loss: 0.6328630697474515, Val Loss: 0.7199454494687014\n",
      "Epoch 8/10, Train Loss: 0.6003791517636128, Val Loss: 0.6894269903991804\n",
      "Epoch 9/10, Train Loss: 0.5808715683006289, Val Loss: 0.7115949100359927\n",
      "Epoch 10/10, Train Loss: 0.5634153442378353, Val Loss: 0.7720921026586101\n",
      "\n",
      "Training NeuralNetwork with lr=0.001, optimizer=momentum, batch norm=False\n",
      "Epoch 1/10, Train Loss: 1.9232909860753655, Val Loss: 1.5823067340046861\n",
      "Epoch 2/10, Train Loss: 1.3881329245995404, Val Loss: 1.2559536875680435\n",
      "Epoch 3/10, Train Loss: 1.1642948885510984, Val Loss: 1.1068820232568786\n",
      "Epoch 4/10, Train Loss: 1.0468240796850803, Val Loss: 1.0199938704108082\n",
      "Epoch 5/10, Train Loss: 0.9700896200396474, Val Loss: 0.9660035953965298\n",
      "Epoch 6/10, Train Loss: 0.9139386350079962, Val Loss: 0.9205379121352074\n",
      "Epoch 7/10, Train Loss: 0.8702709613892801, Val Loss: 0.8877687188255232\n",
      "Epoch 8/10, Train Loss: 0.8315067435440577, Val Loss: 0.860785766569681\n",
      "Epoch 9/10, Train Loss: 0.7975280983118048, Val Loss: 0.8319663518909798\n",
      "Epoch 10/10, Train Loss: 0.7684418964861635, Val Loss: 0.8105646657735802\n",
      "\n",
      "Training NeuralNetwork with lr=0.01, optimizer=adam, batch norm=False\n",
      "Epoch 1/10, Train Loss: 2.1891276205418415, Val Loss: 1.7062780803372695\n",
      "Epoch 2/10, Train Loss: 1.4821687030821964, Val Loss: 1.8731124335250189\n",
      "Epoch 3/10, Train Loss: 1.5364660153959755, Val Loss: 2.263512603072233\n",
      "Epoch 4/10, Train Loss: 1.5351737031615582, Val Loss: 1.7257773680395858\n",
      "Epoch 5/10, Train Loss: 1.6954788843592503, Val Loss: 1.7308738070518472\n",
      "Epoch 6/10, Train Loss: 1.4474357551990304, Val Loss: 1.3600212262813436\n",
      "Epoch 7/10, Train Loss: 1.5559537791626115, Val Loss: 1.6994579432662142\n",
      "Epoch 8/10, Train Loss: 1.4091183162686831, Val Loss: 1.6420996443823326\n",
      "Epoch 9/10, Train Loss: 1.4993872091062646, Val Loss: 1.817770020733046\n",
      "Epoch 10/10, Train Loss: 1.4413321307323816, Val Loss: 1.3462410136017688\n",
      "\n",
      "Training NeuralNetwork with lr=0.01, optimizer=momentum, batch norm=False\n",
      "Epoch 1/10, Train Loss: 1.3169760640125323, Val Loss: 1.0189300059925679\n",
      "Epoch 2/10, Train Loss: 0.9084285393988997, Val Loss: 0.8788112791818242\n",
      "Epoch 3/10, Train Loss: 0.795717242018243, Val Loss: 0.8826972838751105\n",
      "Epoch 4/10, Train Loss: 0.7183563348807004, Val Loss: 0.7562433872805085\n",
      "Epoch 5/10, Train Loss: 0.6739260470332052, Val Loss: 0.7540346728854401\n",
      "Epoch 6/10, Train Loss: 0.6323879806329484, Val Loss: 0.697475567849916\n",
      "Epoch 7/10, Train Loss: 0.6060308306814727, Val Loss: 0.703117220294337\n",
      "Epoch 8/10, Train Loss: 0.5853338078258935, Val Loss: 0.689231866407533\n",
      "Epoch 9/10, Train Loss: 0.5578846386506076, Val Loss: 0.686117171981307\n",
      "Epoch 10/10, Train Loss: 0.5462159981342622, Val Loss: 0.71902717060821\n",
      "\n",
      "Training NeuralNetwork with lr=0.001, optimizer=adam, batch norm=True\n",
      "Epoch 1/10, Train Loss: 1.487420904740431, Val Loss: 1.167830377817154\n",
      "Epoch 2/10, Train Loss: 1.0393446443086847, Val Loss: 0.9558829102058743\n",
      "Epoch 3/10, Train Loss: 0.9219689838160898, Val Loss: 0.8927649440633696\n",
      "Epoch 4/10, Train Loss: 0.8333924477014161, Val Loss: 0.8154409868252832\n",
      "Epoch 5/10, Train Loss: 0.7793454310468902, Val Loss: 0.7817515606103942\n",
      "Epoch 6/10, Train Loss: 0.7390091735079996, Val Loss: 0.7575868324831475\n",
      "Epoch 7/10, Train Loss: 0.7160152977615818, Val Loss: 0.7722367466708948\n",
      "Epoch 8/10, Train Loss: 0.7016797008880059, Val Loss: 0.7345208339393139\n",
      "Epoch 9/10, Train Loss: 0.6677486661477874, Val Loss: 0.7382521779211455\n",
      "Epoch 10/10, Train Loss: 0.6511065095217151, Val Loss: 0.7153677566973276\n",
      "\n",
      "Training NeuralNetwork with lr=0.001, optimizer=momentum, batch norm=True\n",
      "Epoch 1/10, Train Loss: 1.6541047684866887, Val Loss: 1.238197989068752\n",
      "Epoch 2/10, Train Loss: 1.1288522391247928, Val Loss: 1.0107809487123822\n",
      "Epoch 3/10, Train Loss: 0.9835405624715468, Val Loss: 0.9366181246763052\n",
      "Epoch 4/10, Train Loss: 0.8945425481273052, Val Loss: 0.8730143752208975\n",
      "Epoch 5/10, Train Loss: 0.836755658028429, Val Loss: 0.823515308111213\n",
      "Epoch 6/10, Train Loss: 0.7875165663232233, Val Loss: 0.7993264979747838\n",
      "Epoch 7/10, Train Loss: 0.7489521109552455, Val Loss: 0.7716124896393266\n",
      "Epoch 8/10, Train Loss: 0.7233503631448508, Val Loss: 0.728484472389831\n",
      "Epoch 9/10, Train Loss: 0.6910434552558937, Val Loss: 0.7280177450630554\n",
      "Epoch 10/10, Train Loss: 0.6717514095832582, Val Loss: 0.7240627422755541\n",
      "\n",
      "Training NeuralNetwork with lr=0.01, optimizer=adam, batch norm=True\n",
      "Epoch 1/10, Train Loss: 1.560680665354479, Val Loss: 1.16967354576255\n",
      "Epoch 2/10, Train Loss: 1.1384312926682452, Val Loss: 1.026318756646888\n",
      "Epoch 3/10, Train Loss: 1.0207634719827228, Val Loss: 0.9944828587562539\n",
      "Epoch 4/10, Train Loss: 0.9341568544767147, Val Loss: 0.8744738022255343\n",
      "Epoch 5/10, Train Loss: 0.8674621624022053, Val Loss: 0.836312841017579\n",
      "Epoch 6/10, Train Loss: 0.8254321043628112, Val Loss: 0.774024827102589\n",
      "Epoch 7/10, Train Loss: 0.7968336683036086, Val Loss: 0.7696388178440028\n",
      "Epoch 8/10, Train Loss: 0.7743587581965692, Val Loss: 0.7633320956036102\n",
      "Epoch 9/10, Train Loss: 0.748279932616952, Val Loss: 0.7862214370695657\n",
      "Epoch 10/10, Train Loss: 0.7324411750880263, Val Loss: 0.7890518919326538\n",
      "\n",
      "Training NeuralNetwork with lr=0.01, optimizer=momentum, batch norm=True\n",
      "Epoch 1/10, Train Loss: 1.4729630060475367, Val Loss: 1.1953521232272304\n",
      "Epoch 2/10, Train Loss: 1.0582162459889553, Val Loss: 0.9533800766911618\n",
      "Epoch 3/10, Train Loss: 0.931386411338673, Val Loss: 0.865945266949576\n",
      "Epoch 4/10, Train Loss: 0.8443595436297153, Val Loss: 0.7976359956313012\n",
      "Epoch 5/10, Train Loss: 0.8002076308774829, Val Loss: 0.7817699907477512\n",
      "Epoch 6/10, Train Loss: 0.7501706517768323, Val Loss: 0.7548838207541511\n",
      "Epoch 7/10, Train Loss: 0.7247423628098947, Val Loss: 0.7399941925392595\n",
      "Epoch 8/10, Train Loss: 0.7020702409625351, Val Loss: 0.7209603927682998\n",
      "Epoch 9/10, Train Loss: 0.6791103679194415, Val Loss: 0.7211886353270952\n",
      "Epoch 10/10, Train Loss: 0.6586881221975769, Val Loss: 0.7206478444642799\n",
      "\n",
      "Best configurations:\n",
      "  model_type     lr optimizer  \\\n",
      "4         BN  0.001      adam   \n",
      "3       NoBN  0.010  momentum   \n",
      "7         BN  0.010  momentum   \n",
      "5         BN  0.001  momentum   \n",
      "0       NoBN  0.001      adam   \n",
      "\n",
      "                                          train_loss  \\\n",
      "4  [1.487420904740431, 1.0393446443086847, 0.9219...   \n",
      "3  [1.3169760640125323, 0.9084285393988997, 0.795...   \n",
      "7  [1.4729630060475367, 1.0582162459889553, 0.931...   \n",
      "5  [1.6541047684866887, 1.1288522391247928, 0.983...   \n",
      "0  [1.3897412085622327, 0.9446058612214657, 0.820...   \n",
      "\n",
      "                                            val_loss  final_train_loss  \\\n",
      "4  [1.167830377817154, 0.9558829102058743, 0.8927...          0.651107   \n",
      "3  [1.0189300059925679, 0.8788112791818242, 0.882...          0.546216   \n",
      "7  [1.1953521232272304, 0.9533800766911618, 0.865...          0.658688   \n",
      "5  [1.238197989068752, 1.0107809487123822, 0.9366...          0.671751   \n",
      "0  [1.13796490915986, 0.9425010339811791, 0.92163...          0.563415   \n",
      "\n",
      "   final_val_loss  \n",
      "4        0.715368  \n",
      "3        0.719027  \n",
      "7        0.720648  \n",
      "5        0.724063  \n",
      "0        0.772092  \n"
     ]
    }
   ],
   "source": [
    "def get_optimizer(name, params, lr, **kwargs):\n",
    "    optimizers = {\n",
    "        'adam': lambda: AdamOptimizer(params, lr=lr, **kwargs),\n",
    "        'momentum': lambda: optim.SGD(params, lr=lr, momentum=0.9, **kwargs),\n",
    "        'sgd': lambda: optim.SGD(params, lr=lr, **kwargs)\n",
    "    }\n",
    "    \n",
    "    return optimizers[name]()\n",
    "\n",
    "def run_experiments(models, lrs, optimizers, train_loader, val_loader, num_epochs=10):\n",
    "    results = []\n",
    "    \n",
    "    for model_config, lr, optim_name in product(models, lrs, optimizers):\n",
    "        model = NeuralNetwork(batch_normalization=model_config['batch_norm'])\n",
    "        model = model.to(device)\n",
    "        \n",
    "        if model_config.get('seed'):\n",
    "            torch.manual_seed(model_config['seed'])\n",
    "            model.apply(init_weights)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = get_optimizer(optim_name, model.parameters(), lr)\n",
    "        \n",
    "        print(f\"\\nTraining {model.__class__.__name__} \"\n",
    "              f\"with lr={lr}, optimizer={optim_name}, \"\n",
    "              f\"batch norm={model_config['batch_norm']}\")\n",
    "        \n",
    "        try:\n",
    "            train_loss, val_loss = train(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                criterion=criterion,\n",
    "                optimizer=optimizer,\n",
    "                epochs=num_epochs\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'model_type': 'BN' if model_config['batch_norm'] else 'NoBN',\n",
    "                'lr': lr,\n",
    "                'optimizer': optim_name,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'final_train_loss': train_loss[-1],\n",
    "                'final_val_loss': val_loss[-1]\n",
    "            })\n",
    "            \n",
    "            plot_losses(train_loss, val_loss, model_config, lr, optim_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Experiment failed: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def plot_losses(train_loss, val_loss, model_config, lr, optim_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss, label='Train Loss', marker='o')\n",
    "    plt.plot(val_loss, label='Val Loss', marker='s')\n",
    "    plt.title(f\"BN={model_config['batch_norm']}, lr={lr}, Optim={optim_name}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"loss_BN_{model_config['batch_norm']}_lr_{lr}_{optim_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "experiment_config = {\n",
    "    'models': [{'batch_norm': False, 'seed': 42}, \n",
    "               {'batch_norm': True, 'seed': 42}],\n",
    "    'lrs': [0.001, 0.01],\n",
    "    'optimizers': ['adam', 'momentum'],\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "results_df = run_experiments(\n",
    "    models=experiment_config['models'],\n",
    "    lrs=experiment_config['lrs'],\n",
    "    optimizers=experiment_config['optimizers'],\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=experiment_config['num_epochs']\n",
    ")\n",
    "\n",
    "print(\"\\nBest configurations:\")\n",
    "print(results_df.sort_values('final_val_loss').head())\n",
    "\n",
    "results_df.to_csv('experiment_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836d6629-0755-453a-9bdf-84f1f02250cc",
   "metadata": {},
   "source": [
    "## Результаты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71cdf784-60d4-4840-a566-f1d3e9289eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>lr</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>final_train_loss</th>\n",
       "      <th>final_val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NoBN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1.3897412085622327, 0.9446058612214657, 0.820...</td>\n",
       "      <td>[1.13796490915986, 0.9425010339811791, 0.92163...</td>\n",
       "      <td>0.563415</td>\n",
       "      <td>0.772092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NoBN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>momentum</td>\n",
       "      <td>[1.9232909860753655, 1.3881329245995404, 1.164...</td>\n",
       "      <td>[1.5823067340046861, 1.2559536875680435, 1.106...</td>\n",
       "      <td>0.768442</td>\n",
       "      <td>0.810565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NoBN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>adam</td>\n",
       "      <td>[2.1891276205418415, 1.4821687030821964, 1.536...</td>\n",
       "      <td>[1.7062780803372695, 1.8731124335250189, 2.263...</td>\n",
       "      <td>1.441332</td>\n",
       "      <td>1.346241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NoBN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>momentum</td>\n",
       "      <td>[1.3169760640125323, 0.9084285393988997, 0.795...</td>\n",
       "      <td>[1.0189300059925679, 0.8788112791818242, 0.882...</td>\n",
       "      <td>0.546216</td>\n",
       "      <td>0.719027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1.487420904740431, 1.0393446443086847, 0.9219...</td>\n",
       "      <td>[1.167830377817154, 0.9558829102058743, 0.8927...</td>\n",
       "      <td>0.651107</td>\n",
       "      <td>0.715368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BN</td>\n",
       "      <td>0.001</td>\n",
       "      <td>momentum</td>\n",
       "      <td>[1.6541047684866887, 1.1288522391247928, 0.983...</td>\n",
       "      <td>[1.238197989068752, 1.0107809487123822, 0.9366...</td>\n",
       "      <td>0.671751</td>\n",
       "      <td>0.724063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>adam</td>\n",
       "      <td>[1.560680665354479, 1.1384312926682452, 1.0207...</td>\n",
       "      <td>[1.16967354576255, 1.026318756646888, 0.994482...</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.789052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BN</td>\n",
       "      <td>0.010</td>\n",
       "      <td>momentum</td>\n",
       "      <td>[1.4729630060475367, 1.0582162459889553, 0.931...</td>\n",
       "      <td>[1.1953521232272304, 0.9533800766911618, 0.865...</td>\n",
       "      <td>0.658688</td>\n",
       "      <td>0.720648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type     lr optimizer  \\\n",
       "0       NoBN  0.001      adam   \n",
       "1       NoBN  0.001  momentum   \n",
       "2       NoBN  0.010      adam   \n",
       "3       NoBN  0.010  momentum   \n",
       "4         BN  0.001      adam   \n",
       "5         BN  0.001  momentum   \n",
       "6         BN  0.010      adam   \n",
       "7         BN  0.010  momentum   \n",
       "\n",
       "                                          train_loss  \\\n",
       "0  [1.3897412085622327, 0.9446058612214657, 0.820...   \n",
       "1  [1.9232909860753655, 1.3881329245995404, 1.164...   \n",
       "2  [2.1891276205418415, 1.4821687030821964, 1.536...   \n",
       "3  [1.3169760640125323, 0.9084285393988997, 0.795...   \n",
       "4  [1.487420904740431, 1.0393446443086847, 0.9219...   \n",
       "5  [1.6541047684866887, 1.1288522391247928, 0.983...   \n",
       "6  [1.560680665354479, 1.1384312926682452, 1.0207...   \n",
       "7  [1.4729630060475367, 1.0582162459889553, 0.931...   \n",
       "\n",
       "                                            val_loss  final_train_loss  \\\n",
       "0  [1.13796490915986, 0.9425010339811791, 0.92163...          0.563415   \n",
       "1  [1.5823067340046861, 1.2559536875680435, 1.106...          0.768442   \n",
       "2  [1.7062780803372695, 1.8731124335250189, 2.263...          1.441332   \n",
       "3  [1.0189300059925679, 0.8788112791818242, 0.882...          0.546216   \n",
       "4  [1.167830377817154, 0.9558829102058743, 0.8927...          0.651107   \n",
       "5  [1.238197989068752, 1.0107809487123822, 0.9366...          0.671751   \n",
       "6  [1.16967354576255, 1.026318756646888, 0.994482...          0.732441   \n",
       "7  [1.1953521232272304, 0.9533800766911618, 0.865...          0.658688   \n",
       "\n",
       "   final_val_loss  \n",
       "0        0.772092  \n",
       "1        0.810565  \n",
       "2        1.346241  \n",
       "3        0.719027  \n",
       "4        0.715368  \n",
       "5        0.724063  \n",
       "6        0.789052  \n",
       "7        0.720648  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('experiment_results.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac06786-4a20-401d-a56e-a758375d5ce1",
   "metadata": {},
   "source": [
    "Модель с использованием Batch Normalization и при learning rate = 0.001 с оптимизатором Adam показала наилучший результат (train: 0.651107, val: 0.715368)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
